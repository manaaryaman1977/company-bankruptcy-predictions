{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9Q7SK2u1wuC"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "class Node:\n",
        "    \"\"\"Decision tree node class\"\"\"\n",
        "    def __init__(self, feature=None, threshold=None, left=None, right=None, *, value=None):\n",
        "        self.feature = feature    # Index of feature to split on\n",
        "        self.threshold = threshold  # Threshold value for split\n",
        "        self.left = left          # Left child node\n",
        "        self.right = right        # Right child node\n",
        "        self.value = value       # Value if leaf node (class probability)\n",
        "\n",
        "\n",
        "class DecisionTree:\n",
        "    \"\"\"Decision tree classifier implementation\"\"\"\n",
        "    def __init__(self, max_depth=10, min_samples_leaf=5, max_features=None):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_leaf = min_samples_leaf\n",
        "        self.max_features = max_features\n",
        "        self.root = None\n",
        "\n",
        "    def _gini_impurity(self, y):\n",
        "        \"\"\"Calculate Gini impurity for a set of labels\"\"\"\n",
        "        counts = np.bincount(y)\n",
        "        probabilities = counts / len(y)\n",
        "        return 1 - np.sum(probabilities ** 2)\n",
        "\n",
        "    def _best_split(self, X, y, feature_indices):\n",
        "        \"\"\"Find the best split for a node using Gini impurity\"\"\"\n",
        "        best_gini = float('inf')\n",
        "        best_feature, best_threshold = None, None\n",
        "\n",
        "        for feature in feature_indices:\n",
        "            thresholds = np.unique(X[:, feature])\n",
        "            for threshold in thresholds:\n",
        "                # Split data\n",
        "                left_mask = X[:, feature] <= threshold\n",
        "                right_mask = ~left_mask\n",
        "\n",
        "                if np.sum(left_mask) < self.min_samples_leaf or np.sum(right_mask) < self.min_samples_leaf:\n",
        "                    continue\n",
        "\n",
        "                # Calculate weighted Gini impurity\n",
        "                gini_left = self._gini_impurity(y[left_mask])\n",
        "                gini_right = self._gini_impurity(y[right_mask])\n",
        "                weighted_gini = (len(y[left_mask]) * gini_left + len(y[right_mask]) * gini_right) / len(y)\n",
        "\n",
        "                if weighted_gini < best_gini:\n",
        "                    best_gini = weighted_gini\n",
        "                    best_feature = feature\n",
        "                    best_threshold = threshold\n",
        "\n",
        "        return best_feature, best_threshold\n",
        "\n",
        "    def _build_tree(self, X, y, depth=0):\n",
        "        \"\"\"Recursively build the decision tree\"\"\"\n",
        "        # Stopping conditions\n",
        "        if (depth >= self.max_depth or\n",
        "            len(y) < self.min_samples_leaf * 2 or\n",
        "            len(np.unique(y)) == 1):\n",
        "            counter = Counter(y)\n",
        "            total = sum(counter.values())\n",
        "            probabilities = {k: v/total for k, v in counter.items()}\n",
        "            return Node(value=probabilities)\n",
        "\n",
        "        # Determine number of features to consider\n",
        "        n_features = X.shape[1]\n",
        "        if isinstance(self.max_features, str):\n",
        "            if self.max_features == 'sqrt':\n",
        "                max_features = int(np.sqrt(n_features))\n",
        "            elif self.max_features == 'log2':\n",
        "                max_features = int(np.log2(n_features))\n",
        "            else:\n",
        "                max_features = n_features\n",
        "        else:\n",
        "            max_features = self.max_features if self.max_features else n_features\n",
        "\n",
        "        # Random feature selection\n",
        "        feature_indices = np.random.choice(\n",
        "            n_features,\n",
        "            size=max_features,\n",
        "            replace=False\n",
        "        )\n",
        "\n",
        "        # Find best split\n",
        "        feature, threshold = self._best_split(X, y, feature_indices)\n",
        "        if feature is None:\n",
        "            counter = Counter(y)\n",
        "            total = sum(counter.values())\n",
        "            probabilities = {k: v/total for k, v in counter.items()}\n",
        "            return Node(value=probabilities)\n",
        "\n",
        "        # Split data\n",
        "        left_mask = X[:, feature] <= threshold\n",
        "        right_mask = ~left_mask\n",
        "\n",
        "        # Recursively build left and right subtrees\n",
        "        left = self._build_tree(X[left_mask], y[left_mask], depth + 1)\n",
        "        right = self._build_tree(X[right_mask], y[right_mask], depth + 1)\n",
        "\n",
        "        return Node(feature=feature, threshold=threshold, left=left, right=right)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Train the decision tree\"\"\"\n",
        "        self.root = self._build_tree(X, y)\n",
        "\n",
        "    def _traverse_tree(self, x, node):\n",
        "        \"\"\"Traverse the tree to make a prediction for a single sample\"\"\"\n",
        "        if node.value is not None:\n",
        "            return node.value\n",
        "\n",
        "        if x[node.feature] <= node.threshold:\n",
        "            return self._traverse_tree(x, node.left)\n",
        "        else:\n",
        "            return self._traverse_tree(x, node.right)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"Predict class probabilities\"\"\"\n",
        "        return np.array([self._traverse_tree(x, self.root) for x in X])\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predict class labels\"\"\"\n",
        "        probas = self.predict_proba(X)\n",
        "        return np.array([max(p.keys(), key=lambda k: p[k]) for p in probas])\n",
        "\n",
        "class RandomForest:\n",
        "    \"\"\"Random Forest classifier implementation\"\"\"\n",
        "    def __init__(self, n_trees=100, max_depth=10, min_samples_leaf=5,\n",
        "                 max_features='sqrt', bootstrap_features=False):\n",
        "        self.n_trees = n_trees\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_leaf = min_samples_leaf\n",
        "        self.max_features = max_features\n",
        "        self.bootstrap_features = bootstrap_features\n",
        "        self.trees = []\n",
        "\n",
        "    def _bootstrap_sample(self, X, y):\n",
        "        \"\"\"Create bootstrap sample of the data\"\"\"\n",
        "        n_samples = X.shape[0]\n",
        "        indices = np.random.choice(n_samples, size=n_samples, replace=True)\n",
        "        return X[indices], y[indices]\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Train the random forest\"\"\"\n",
        "        self.trees = []\n",
        "        for _ in range(self.n_trees):\n",
        "            tree = DecisionTree(\n",
        "                max_depth=self.max_depth,\n",
        "                min_samples_leaf=self.min_samples_leaf,\n",
        "                max_features=self.max_features\n",
        "            )\n",
        "\n",
        "            # Bootstrap sample\n",
        "            X_sample, y_sample = self._bootstrap_sample(X, y)\n",
        "\n",
        "            # Optionally bootstrap features\n",
        "            if self.bootstrap_features:\n",
        "                n_features = X.shape[1]\n",
        "                if isinstance(self.max_features, str):\n",
        "                    if self.max_features == 'sqrt':\n",
        "                        max_features = int(np.sqrt(n_features))\n",
        "                    elif self.max_features == 'log2':\n",
        "                        max_features = int(np.log2(n_features))\n",
        "                    else:\n",
        "                        max_features = n_features\n",
        "                else:\n",
        "                    max_features = self.max_features if self.max_features else n_features\n",
        "\n",
        "                feature_indices = np.random.choice(\n",
        "                    n_features,\n",
        "                    size=max_features,\n",
        "                    replace=True\n",
        "                )\n",
        "                X_sample = X_sample[:, feature_indices]\n",
        "                tree.feature_indices = feature_indices\n",
        "\n",
        "            tree.fit(X_sample, y_sample)\n",
        "            self.trees.append(tree)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"Predict class probabilities (soft voting)\"\"\"\n",
        "        tree_probas = []\n",
        "        for tree in self.trees:\n",
        "            if hasattr(tree, 'feature_indices'):\n",
        "                X_subset = X[:, tree.feature_indices]\n",
        "                proba = tree.predict_proba(X_subset)\n",
        "            else:\n",
        "                proba = tree.predict_proba(X)\n",
        "            tree_probas.append(proba)\n",
        "\n",
        "        # Average probabilities across all trees\n",
        "        avg_proba = []\n",
        "        for i in range(len(X)):\n",
        "            class_probs = {}\n",
        "            for proba in tree_probas:\n",
        "                for cls, p in proba[i].items():\n",
        "                    class_probs[cls] = class_probs.get(cls, 0) + p\n",
        "            total = sum(class_probs.values())\n",
        "            avg_proba.append({k: v/total for k, v in class_probs.items()})\n",
        "\n",
        "        return avg_proba\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predict class labels (hard voting)\"\"\"\n",
        "        probas = self.predict_proba(X)\n",
        "        return np.array([max(p.keys(), key=lambda k: p[k]) for p in probas])\n",
        "\n",
        "    def get_feature_importances(self, feature_names=None):\n",
        "        \"\"\"Calculate feature importances based on Gini importance\"\"\"\n",
        "        if not self.trees:\n",
        "            return None\n",
        "\n",
        "        if feature_names is None:\n",
        "            n_features = self.trees[0].max_features if hasattr(self.trees[0], 'max_features') else X.shape[1]\n",
        "            feature_names = range(n_features)\n",
        "\n",
        "        importances = {f: 0 for f in feature_names}\n",
        "        total_importances = 0\n",
        "\n",
        "        for tree in self.trees:\n",
        "            # Get feature indices used in this tree\n",
        "            if hasattr(tree, 'feature_indices'):\n",
        "                features_used = tree.feature_indices\n",
        "            else:\n",
        "                features_used = range(len(feature_names))\n",
        "\n",
        "            # Calculate importance for each feature in this tree\n",
        "            tree_importances = self._compute_tree_importance(tree.root, len(feature_names))\n",
        "            for f_idx, imp in enumerate(tree_importances):\n",
        "                if f_idx in features_used:\n",
        "                    importances[feature_names[f_idx]] += imp\n",
        "                    total_importances += imp\n",
        "\n",
        "        # Normalize importances\n",
        "        if total_importances > 0:\n",
        "            importances = {k: v/total_importances for k, v in importances.items()}\n",
        "\n",
        "        return importances\n",
        "\n",
        "    def _compute_tree_importance(self, node, n_features):\n",
        "        \"\"\"Recursively compute feature importance for a single tree\"\"\"\n",
        "        importances = np.zeros(n_features)\n",
        "\n",
        "        if node.value is not None:  # Leaf node\n",
        "            return importances\n",
        "\n",
        "        # Calculate importance for this node's feature\n",
        "        left_counts = Counter([max(p.keys(), key=lambda k: p[k]) for p in node.left.predict_proba])\n",
        "        right_counts = Counter([max(p.keys(), key=lambda k: p[k]) for p in node.right.predict_proba])\n",
        "        parent_impurity = self._gini_impurity(list(left_counts.elements()) + list(right_counts.elements()))\n",
        "        left_impurity = self._gini_impurity(list(left_counts.elements()))\n",
        "        right_impurity = self._gini_impurity(list(right_counts.elements()))\n",
        "        impurity_reduction = parent_impurity - (left_impurity + right_impurity)/2\n",
        "\n",
        "        importances[node.feature] = impurity_reduction\n",
        "\n",
        "        # Add importances from child nodes\n",
        "        left_importances = self._compute_tree_importance(node.left, n_features)\n",
        "        right_importances = self._compute_tree_importance(node.right, n_features)\n",
        "\n",
        "        return importances + left_importances + right_importances\n",
        "    def summary(self):\n",
        "        \"\"\"Output a summary of the Random Forest model\"\"\"\n",
        "        print(\"Random Forest Model Summary:\")\n",
        "        print(f\"Number of Trees: {self.n_trees}\")\n",
        "        print(f\"Max Depth: {self.max_depth}\")\n",
        "        print(f\"Min Samples per Leaf: {self.min_samples_leaf}\")\n",
        "        print(f\"Max Features: {self.max_features}\")\n",
        "        print(f\"Bootstrap Features: {self.bootstrap_features}\")\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage with the bankruptcy dataset\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load and prepare data\n",
        "data = pd.read_csv('Train.csv')\n",
        "X = data.drop('Bankrupt?', axis=1).values\n",
        "y = data['Bankrupt?'].values\n",
        "feature_names = data.drop('Bankrupt?', axis=1).columns.tolist()\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n"
      ],
      "metadata": {
        "id": "bFJi2Svn10ME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "smote = SMOTE(sampling_strategy='minority', random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)"
      ],
      "metadata": {
        "id": "Vy_W5ZQq2t4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "rfnew = RandomForest(\n",
        "    n_trees=100,\n",
        "    max_depth=10,\n",
        "    min_samples_leaf=5,\n",
        "    max_features='sqrt',  # Now accepts 'sqrt' or 'log2' or integer\n",
        "    bootstrap_features=True\n",
        ")\n",
        "rfnew.fit(X_resampled, y_resampled)"
      ],
      "metadata": {
        "id": "bpiKF3WS2vjM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_prednew = rfnew.predict(X_test)\n",
        "print(classification_report(y_test, y_prednew))"
      ],
      "metadata": {
        "id": "frAxbicS22kZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import precision_score, recall_score, confusion_matrix\n",
        "\n",
        "# Assuming you have:\n",
        "# y_proba = list of dictionaries with class probabilities like:\n",
        "y_proba = rfnew.predict_proba(X_test)\n",
        "# [{0: 0.989, 1: 0.011}, {0: 0.988, 1: 0.012}, ...]\n",
        "# y_test = true labels (0 or 1)\n",
        "thresholds = np.linspace(0.1, 0.9, 10)\n",
        "\n",
        "# Extract probabilities for class 1\n",
        "y_proba_class1 = np.array([d[1] for d in y_proba])\n",
        "\n",
        "# Initialize lists to store metrics\n",
        "thresholds = np.linspace(0.1, 0.9, 10)\n",
        "precisions = []\n",
        "recalls = []\n",
        "f1_scores = []\n",
        "confusion_matrices = []\n",
        "\n",
        "print(\"Testing different thresholds to improve recall for class 1:\\n\")\n",
        "\n",
        "for thresh in thresholds:\n",
        "    # Apply threshold to get predicted class (1 if >= threshold, else 0)\n",
        "    y_pred = (y_proba_class1 >= thresh).astype(int)\n",
        "\n",
        "    # Calculate metrics\n",
        "    precision = precision_score(y_test, y_pred, pos_label=1)\n",
        "    recall = recall_score(y_test, y_pred, pos_label=1)\n",
        "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    # Store results\n",
        "    precisions.append(precision)\n",
        "    recalls.append(recall)\n",
        "    f1_scores.append(f1)\n",
        "    confusion_matrices.append(cm)\n",
        "\n",
        "    # Print current threshold results\n",
        "    print(f\"Threshold: {thresh:.2f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1-score: {f1:.4f}\")\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(cm)\n",
        "    print(\"-\"*40)\n",
        "\n",
        "# Plot precision-recall curve\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Precision-Recall Curve\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(thresholds, precisions, 'b-', label='Precision')\n",
        "plt.plot(thresholds, recalls, 'r-', label='Recall')\n",
        "plt.plot(thresholds, f1_scores, 'g-', label='F1-score')\n",
        "plt.xlabel('Threshold')\n",
        "plt.ylabel('Score')\n",
        "plt.title('Metrics vs Threshold')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Precision-Recall Tradeoff\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(recalls, precisions, 'mo-')\n",
        "for i, thresh in enumerate(thresholds):\n",
        "    plt.annotate(f\"{thresh:.2f}\", (recalls[i], precisions[i]),\n",
        "                 textcoords=\"offset points\", xytext=(5,5), ha='center')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Find optimal threshold based on F1-score\n",
        "optimal_idx = np.argmax(f1_scores)\n",
        "optimal_threshold = thresholds[optimal_idx]\n",
        "print(f\"\\nOptimal threshold: {optimal_threshold:.2f}\")\n",
        "print(f\"At this threshold - Precision: {precisions[optimal_idx]:.4f}, Recall: {recalls[optimal_idx]:.4f}, F1: {f1_scores[optimal_idx]:.4f}\")"
      ],
      "metadata": {
        "id": "5TFT6Z1S29jy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_with_threshold = (y_proba_class1 >= 0.63).astype(int)\n",
        "print(classification_report(y_test, y_pred_with_threshold))"
      ],
      "metadata": {
        "id": "PoiODzDE2_rs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rfnew.summary()"
      ],
      "metadata": {
        "id": "qwy2-oLA3dkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Saving the model to a file\n",
        "with open('random_forest_model.pt', 'wb') as file:\n",
        "    pickle.dump(rfnew, file)\n",
        "\n"
      ],
      "metadata": {
        "id": "0CmwPbKF3GdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('random_forest_model.pt', 'rb') as file:\n",
        "    loaded_model = pickle.load(file)\n",
        "\n",
        "# Predicting with the loaded model\n",
        "\n",
        "y_proba = loaded_model.predict_proba(X_test)\n",
        "\n",
        "y_pred_with_threshold = (y_proba_class1 >= 0.63).astype(int)\n",
        "print(classification_report(y_test, y_pred_with_threshold))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2Igg0bxv3UGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"scaler.pth\", \"wb\") as f:\n",
        "    pickle.dump(scaler, f)"
      ],
      "metadata": {
        "id": "bq4hl9vAIQiu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H8hBR9IMIX1d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}