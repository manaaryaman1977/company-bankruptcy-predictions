{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VqhZYTI-sImK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "class TreeNode:\n",
        "    def __init__(self, data, grad, hess, indices, col_sample=0.8, min_samples=5, min_hess=1, max_depth=10, reg_lambda=1, min_gain=1, epsilon=0.1):\n",
        "        self.data, self.grad, self.hess = data, grad, hess\n",
        "        self.indices = indices\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples = min_samples\n",
        "        self.reg_lambda = reg_lambda\n",
        "        self.min_gain = min_gain\n",
        "        self.min_hess = min_hess\n",
        "        self.total_samples = len(indices)\n",
        "        self.total_features = data.shape[1]\n",
        "        self.col_sample = col_sample\n",
        "        self.epsilon = epsilon\n",
        "        self.selected_cols = np.random.permutation(self.total_features)[:round(self.col_sample * self.total_features)]\n",
        "        self.value = self.compute_optimal_value(self.grad[self.indices], self.hess[self.indices])\n",
        "        self.best_score = float('-inf')\n",
        "        self.split_feature = None\n",
        "        self.split_value = None\n",
        "        self.left_child = None\n",
        "        self.right_child = None\n",
        "        self.split_node()\n",
        "\n",
        "    def compute_optimal_value(self, grad, hess):\n",
        "        return -np.sum(grad) / (np.sum(hess) + self.reg_lambda)\n",
        "\n",
        "    def split_node(self):\n",
        "        for feature in self.selected_cols:\n",
        "            self.evaluate_split(feature)\n",
        "        if self.is_leaf():\n",
        "            return\n",
        "        feature_data = self.split_feature_data()\n",
        "        left_indices = np.nonzero(feature_data <= self.split_value)[0]\n",
        "        right_indices = np.nonzero(feature_data > self.split_value)[0]\n",
        "        self.left_child = TreeNode(self.data, self.grad, self.hess, self.indices[left_indices], self.col_sample, self.min_samples, self.min_hess, self.max_depth-1, self.reg_lambda, self.min_gain, self.epsilon)\n",
        "        self.right_child = TreeNode(self.data, self.grad, self.hess, self.indices[right_indices], self.col_sample, self.min_samples, self.min_hess, self.max_depth-1, self.reg_lambda, self.min_gain, self.epsilon)\n",
        "\n",
        "    def evaluate_split(self, feature_idx):\n",
        "        feature_data = self.data[self.indices, feature_idx]\n",
        "        for value in np.unique(feature_data):\n",
        "            left_mask = feature_data <= value\n",
        "            right_mask = feature_data > value\n",
        "            if np.sum(left_mask) < self.min_samples or np.sum(right_mask) < self.min_samples:\n",
        "                continue\n",
        "            score = self.calculate_gain(left_mask, right_mask)\n",
        "            if score > self.best_score:\n",
        "                self.best_score = score\n",
        "                self.split_feature = feature_idx\n",
        "                self.split_value = value\n",
        "\n",
        "    def calculate_gain(self, left_mask, right_mask):\n",
        "        grad, hess = self.grad[self.indices], self.hess[self.indices]\n",
        "        grad_left, hess_left = grad[left_mask].sum(), hess[left_mask].sum()\n",
        "        grad_right, hess_right = grad[right_mask].sum(), hess[right_mask].sum()\n",
        "        gain = 0.5 * ((grad_left ** 2 / (hess_left + self.reg_lambda)) + (grad_right ** 2 / (hess_right + self.reg_lambda)) - ((grad_left + grad_right) ** 2 / (hess_left + hess_right + self.reg_lambda))) - self.min_gain\n",
        "        return gain\n",
        "\n",
        "    def is_leaf(self):\n",
        "        return self.best_score == float('-inf') or self.max_depth <= 0\n",
        "\n",
        "    def split_feature_data(self):\n",
        "        return self.data[self.indices, self.split_feature]\n",
        "\n",
        "    def predict_sample(self, sample):\n",
        "        if self.is_leaf():\n",
        "            return self.value\n",
        "        return self.left_child.predict_sample(sample) if sample[self.split_feature] <= self.split_value else self.right_child.predict_sample(sample)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.array([self.predict_sample(sample) for sample in X])\n",
        "\n",
        "\n",
        "class GradientBoostingTree:\n",
        "    def fit(self, data, grad, hess, col_sample=0.8, min_samples=5, min_hess=1, max_depth=10, reg_lambda=1, min_gain=1, epsilon=0.1):\n",
        "        self.tree = TreeNode(data, grad, hess, np.arange(len(data)), col_sample, min_samples, min_hess, max_depth, reg_lambda, min_gain, epsilon)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.tree.predict(X)\n",
        "\n",
        "\n",
        "class GradientBoostingClassifier:\n",
        "    def __init__(self):\n",
        "        self.trees = []\n",
        "        self.base_score = None\n",
        "        self.pred_mean = None\n",
        "\n",
        "    @staticmethod\n",
        "    def sigmoid(x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def compute_gradient(self, preds, labels):\n",
        "        preds = self.sigmoid(preds)\n",
        "        return preds - labels\n",
        "\n",
        "    def compute_hessian(self, preds):\n",
        "        preds = self.sigmoid(preds)\n",
        "        return preds * (1 - preds)\n",
        "\n",
        "    def fit(self, X, y, col_sample=0.8, min_hess=1, max_depth=5, min_samples=5, lr=0.4, rounds=5, reg_lambda=1.5, min_gain=1, epsilon=0.1):\n",
        "        self.base_score = np.full(X.shape[0], 1, dtype=np.float64)\n",
        "\n",
        "        # Compute class weights\n",
        "        n_total = len(y)\n",
        "        n_class_0 = np.sum(y == 0)\n",
        "        n_class_1 = np.sum(y == 1)\n",
        "\n",
        "        w_0 = n_class_1 / n_total if n_class_0 > 0 else 1\n",
        "        w_1 = n_class_0 / n_total if n_class_1 > 0 else 1\n",
        "\n",
        "        class_weights = np.where(y == 0, w_0, w_1)  # Assign weights per sample\n",
        "\n",
        "        for _ in range(rounds):\n",
        "            grad = self.compute_gradient(self.base_score, y) * class_weights\n",
        "            hess = self.compute_hessian(self.base_score) * class_weights\n",
        "\n",
        "            tree = GradientBoostingTree().fit(X, grad, hess, col_sample, min_samples, min_hess, max_depth, reg_lambda, min_gain, epsilon)\n",
        "            self.base_score += lr * tree.predict(X)\n",
        "            self.trees.append(tree)\n",
        "\n",
        "        # Compute weighted mean for pred_mean\n",
        "        predictions = self.predict_proba(X)\n",
        "        self.pred_mean = np.average(predictions, weights=class_weights)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        predictions = np.zeros(X.shape[0])\n",
        "        for tree in self.trees:\n",
        "            predictions += tree.predict(X)\n",
        "        scores = predictions\n",
        "        return (self.sigmoid(np.full((X.shape[0], 1), 1).flatten().astype('float64') + predictions))\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = self.predict_proba(X)\n",
        "        return (predictions > self.pred_mean).astype(int)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, precision_recall_curve, auc\n",
        "\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"Train.csv\")  # Replace with your actual dataset\n",
        "\n",
        "# Split target and features\n",
        "y = df.iloc[:, 0]   # First column is the target variable\n",
        "X = df.iloc[:, 1:]  # Remaining columns are features\n",
        "\n",
        "# Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=5)\n",
        "\n",
        "# Implementing the specific Isolation Forest model\n",
        "iso_forest = IsolationForest(\n",
        "    n_estimators=310,\n",
        "    contamination=0.08362674314960916,\n",
        "    random_state=42\n",
        ")\n",
        "iso_forest.fit(X_train)\n",
        "\n",
        "# Generate Anomaly Scores as a New Feature (avoiding SettingWithCopyWarning)\n",
        "X_train = X_train.copy()\n",
        "X_test = X_test.copy()\n",
        "X_train[\"anomaly_score\"] = iso_forest.decision_function(X_train)\n",
        "X_test[\"anomaly_score\"] = iso_forest.decision_function(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0oSlJ7401Tz",
        "outputId": "9d6c19b9-fafe-4f33-bc9f-6c1c6b89d993"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 2 folds for each of 5 candidates, totalling 10 fits\n",
            "Best Parameters: {'col_sample': np.float64(0.6464504127199977), 'lr': np.float64(0.5645269111408631), 'max_depth': 7, 'min_gain': np.float64(1.1757488779543146), 'min_hess': 2, 'min_samples': 6, 'reg_lambda': np.float64(2.8555043892121317), 'rounds': 8}\n",
            "Best F1 Score: 0.3964211300404461\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import make_scorer, f1_score\n",
        "from scipy.stats import uniform, randint\n",
        "import numpy as np\n",
        "\n",
        "# Define a parameter distribution\n",
        "param_dist = {\n",
        "    'col_sample': uniform(0.6, 1),  # Values between 0.6 and 1.0\n",
        "    'min_hess': randint(1, 10),\n",
        "    'max_depth': randint(3, 10),\n",
        "    'min_samples': randint(3, 10),\n",
        "    'lr': uniform(0.2, 0.6),  # Values between 0.2 and 0.6\n",
        "    'rounds': randint(3, 10),\n",
        "    'reg_lambda': uniform(0.5, 2.5),  # Values between 0.5 and 2.5\n",
        "    'min_gain': uniform(0.5, 1.5)\n",
        "}\n",
        "\n",
        "# Custom scorer for RandomizedSearchCV\n",
        "scorer = make_scorer(f1_score)\n",
        "\n",
        "# Convert GradientBoostingClassifier into a scikit-learn compatible estimator\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "\n",
        "class GBTWrapper(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, col_sample=0.8, min_hess=1, max_depth=5, min_samples=5, lr=0.4, rounds=5, reg_lambda=1.5, min_gain=1, epsilon=0.1):\n",
        "        self.col_sample = col_sample\n",
        "        self.min_hess = min_hess\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples = min_samples\n",
        "        self.lr = lr\n",
        "        self.rounds = rounds\n",
        "        self.reg_lambda = reg_lambda\n",
        "        self.min_gain = min_gain\n",
        "        self.epsilon = epsilon\n",
        "        self.model = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.model = GradientBoostingClassifier()\n",
        "        self.model.fit(X, y, col_sample=self.col_sample, min_hess=self.min_hess, max_depth=self.max_depth,\n",
        "                       min_samples=self.min_samples, lr=self.lr, rounds=self.rounds, reg_lambda=self.reg_lambda,\n",
        "                       min_gain=self.min_gain, epsilon=self.epsilon)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.model.predict(X)\n",
        "\n",
        "# Perform Randomized Search for faster tuning\n",
        "random_search = RandomizedSearchCV(\n",
        "    GBTWrapper(), param_distributions=param_dist, n_iter=5, scoring=scorer, cv=2, n_jobs=-1, verbose=1, random_state=42\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "random_search.fit(X_train.values, y_train.values)\n",
        "\n",
        "# Print best results\n",
        "print(\"Best Parameters:\", random_search.best_params_)\n",
        "print(\"Best F1 Score:\", random_search.best_score_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6cOJH6V6Xjf"
      },
      "outputs": [],
      "source": [
        "estimator=random_search.best_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "estimator.model.predict_proba(X_test.values)"
      ],
      "metadata": {
        "id": "oZFzdJZ7Mf0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fh9lRIzvCbiM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b6010a9-bcad-41fa-9df5-472c7c630acd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9688\n",
            "F1 Score: 0.4848\n",
            "Precision-Recall AUC: 0.4059\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98      1060\n",
            "           1       0.46      0.52      0.48        31\n",
            "\n",
            "    accuracy                           0.97      1091\n",
            "   macro avg       0.72      0.75      0.73      1091\n",
            "weighted avg       0.97      0.97      0.97      1091\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Predict Probabilities\n",
        "y_pred_prob = estimator.model.predict_proba(X_test.values)  # Ensure correct format\n",
        "y_pred_prob = y_pred_prob.flatten()  # Ensure it's 1D\n",
        "\n",
        "# Adjust Decision Threshold\n",
        "threshold = 0.9  # Can be tuned further\n",
        "y_pred = (y_pred_prob >= threshold).astype(int)\n",
        "\n",
        "# Evaluation\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_pred_prob)\n",
        "pr_auc = auc(recall, precision)\n",
        "\n",
        "# Print Results\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(f\"Precision-Recall AUC: {pr_auc:.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I7nfmgz0OqIy"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}